\chapter{Discussion} \label{discussion}

\section{Reflection on RQ 1}


\section{Reflection on RQ 2}
xxxxxDISCUSS PRODUCED MAPSxxxx

\section{Recorded biases} \label{discussion_rec_bias}
Multiple different data-sources have been used in the course of this thesis. All data contain some form of bias given either by its origin, composition, the way it was obtained or processed. 
This section will cover encountered biases and their potential effects on the results as well as their interpretation.

\subsection*{Ground truth}
The conventional data-acquisition methods such as surveys and interviews hold a comparably low reputation of bias compared to techniques working with social media data. Nevertheless, performing 52 interviews revealed certain conceptual biases that will be addressed here. The fact of only performing ground truthing during three days in the same season for only three specific locations in the entire research area will not be further discussed. The limited informative and representative value is undoubted but was constraint by the time expenditure for this task and the duration of this thesis. \\
One major observed problematic was encountered during the selection phase of a possible interviewee in the field. Depending on the activity that person was performing the chance of an active engagement attempt from the side of the author and an positive consent response from the interviewee varied strongly. People that were jogging with headphones for instance were less likely to be interviewed than a person going for a relaxing walk. It is without doubt that this discrepancy in interviewing probability per NBRA-class has an impact on the observed distribution.\\
Secondly, it was noted that to avoid long pauses - where the interviewee gets stuck on a question - certain words, ideas or examples were mentioned to keep the flow of the interview alive. It often occurred that the interviewees were convinced by some of the mentioned words which were then given as answers. These were mostly keywords that described the visitation motivation such as the term \textit{geographic closeness} which occurred in higher frequencies than others. These depicted biases will reflect in the dataset used for evaluating the models legitimacy and the ground truth results. Similar biases were also observed by \textcite{Hanemann2011, Kling2012, Tenerelli2016} regarding the procedure of traditional surveys and interviews.

\subsection*{Model}
It is known that data from social media platforms such as Instagram and Flickr contain various forms of bias given among others by the demography of the user base \parencite{Heikinheimo2017}, the platform purpose, the dominantly uploaded content-type, noise by bots \parencite{Edwards2014} and fluctuations in social media platform popularity that produce delusive content. These general social media data biases are already widely covered in scientific literature \parencite{Ruths2014, Lazer2014, Zook2017} and will not be further discussed here. Data-processing and model setup induced bias however will be addressed since it is specific to the approach used in this thesis. \\

\subsubsection*{Data-processing} Dataset integrated bias such as dominant users and bulk-uploads that distort the general perception of the data were mitigated by subjectively defined and implemented thresholds during the data-processing phase described in section \ref{bias_dominant_authors} and \ref{bias_bulk_uploads} respectively. These thresholds as already mentioned are set by the author which simultaneously adds a subjective view and alteration to every dataset. Also, it is to be expected that these are not the only biases present. Media objects for instance by tourists and locals were treated with equal importance, even though the latter should have a better knowledge over the optimal places to perform certain activities. Additionally, media objects which try to promote e.g. a product, a location, an activity or which are influenced or sponsored by a third party were not specifically filtered out.\\

\subsubsection*{Training's data} The composition, origin and quantity of the training's data is linked to numerous known flaws of which the majority were investigated. All findings will be listed and discussed here. \\
The fact that both models were solely trained on Instagram data but used to predict also Flickr media objects is a known bias. The associated magnitude of this flaw in regards to the performance of M1 and M2 was investigated in table \ref{tab:m1_actual_precision} and table \ref{tab:m2_actual_precision} respectively and proven to be not significant on a 5\% level. \\
The issue of using media objects for training that originate from a different area than the media objects on which the models predict is problematic and doubtful. This decision was not made by choice but rather because of a too small available dataset from the research area. The short data-acquisition period (see section \ref{Instagram_timespan}) and the retirement of the Instagram API on the 11.12.2018 did not allow for enough media objects per NBRA-class to train a sophisticated model. The absence of a significant language difference between the two regions which is considered to be one of the crucial requirements was investigated in section \ref{fig:det_languages} and could not be disproven.
Even with the multiple times bigger dataset from Zurich were some classes such as 'horse riding' with 13 entries (see table \ref{tab:trainingsdata}) poorly represented. This is not a bias in its usual sense but rather a weakness of the model as a whole.

\subsubsection*{Setup} 
The classifications between which the models differentiate were chosen based on the guideline from \textcite{IFL2018}. The manual training's data selection as well as the manual model performance evaluation revealed how alike certain classes are. This similarity was mostly given by various interpretation of observed media object authors on what they think a certain NBRA is. The strongest inconsistency is observed in between the classes 'walking' and 'hiking'. The implementation of ones own definition of these classes is of no use if the content of the media objects do not follow them. The phrase: "I went for a walk on the mountain Matterhorn." should illustrate this problem nicely. A potential author of that phrase obviously described his/her performed activity as 'walking' according to the used verb and his/her perception. Somebody else would associate this phrase rather with the activity 'hiking' due to the mentioned noun 'mountain'. At this point the question arises what should be valued more - the perception of the original author or a set definition by the person evaluating the data? Chances are that the original author processes more information about the situation that is not included in the media object itself. This information might explain why the verb 'walking' was chosen over 'hiking' e.g. because a mountain railway was used. 
Taking this into consideration a merge of these two NBRA-classes could be considered.

\subsubsection*{Predictions}
The spatial distribution accuracy and the associated interpretation bias of Instagram media objects has been discussed in section \ref{instagram_location_tag}. User generated location tags impose the issue of imprecise or simply wrong positions as also described by \textcite{Lee2016}. To mitigate these effects superordinate, general location tags associated with a specific region could be excluded as performed by \textcite{Heikinheimo2017} to increase the overall spatial distribution accuracy of NBRAs.

\section{Model performance} \label{disussion_model_performance}
The comparison between model performances during model-tuning (see M1 - table \ref{tab:m1_linearSVC_bestscores} and M2 - table \ref{tab:m2_linearSVC_bestscores}), model-testing on the entire training's dataset (see figure \ref{tab:m1_m2_linearSVC_final_scores}) and model-validation on new data (see table \ref{precision_unseen_data}) revealed some inconsistency. M1 solely showed a better performance while fitting on fewer training points. This was the case during model-tuning due to the necessary testing subset which accounts for 25\% of the total available data. Besides that M2 showed better final model performance as well as a better performance on new (unseen) data. This discrepancy suggests that the size of the training's dataset used in this thesis was considerably small since 25\% more training's data still led to an significant performance increase for both models but especially for M2. The enhanced M2 performance on unseen data also indicates that M2 possesses a better generalisation capability compared to M1 which is also reflected in the reduced False Negative predictions - indicating a higher M2 recall score.

\subsection*{Comparable model baselines} \label{model_baseline}
The models presented in this thesis can be compared to text-classification models of similar research papers to acquire a baseline reference. Recent work of \textcite{Das2018} presents a text sentiment classification model which is based on NF-IDF (also used in this thesis) in conjunction with an additional algorithm to enhance sentiment prediction. Three different classifiers were tested of which also the linearSVC showed the best performance on used review datasets. The presented 10-fold cross-validated model accuracy on the main IMBD movie review dataset was 89.91\% with a train-test split of 80\% and 20\% respectively.\\
Another reference is the deep-learning based model created by \textcite{Li2018} which is optimised to classify Chinese text. The observed accuracy of the tuned model on the Chinese and English dataset lies at 96.23\% and 94.88\% respectively. The latter should function as better comparison to M1 and M2 from this thesis which achieved a test accuracy of 93.7\% and 88.2\% respectively.

\section{Ethics}
Drawing upon crowed sourced big-data is inevitably shadowed by ethical controversies as shown by \textcite{Taylor2018}. Using GPS locations and other sensitive data to identify and analyse patterns of human behaviour is rightfully criticised and sometimes considered unethical practise \parencite{Taylor2018} which is why ethical guidelines for digital engagement are of increasing importance \parencite{Bowen2013}.\\
Normally, when we talk about UGC we are referring to open-data that is publicly available, data that the user decided to share on the web on his or her own behalf - or is it?. The paper of \textcite{Estima2016} shows that not the entirety of UGC is voluntarily and willingly shared. Did the user sign up for his or her data to be systematically analysed? This depends on the Terms of Service (TOS) of the corresponding service which are hardly ever read by the consumer.\\
Media appearances such as Facebook's experiments in emotion manipulation \parencite{Jouhki2016} which enabled the targeting of easily susceptible teenagers to personalised advertisement\footnote{https://www.independent.co.uk/news/media/facebook-leaked-documents-research-targeted-insecure-youth-teenagers-vulnerable-moods-advertising-a7711551.html, accessed: 07.04.2019} present the power this data possess. Having access to hundreds of personalised media objects of a specific social media user enables analytics to construct a blueprint of that person. This blueprint can encompasses the persons political orientation\footnote{https://www.theguardian.com/media/2010/apr/30/social-media-election-2010, accessed: 07.04.2019}, its home and work location, preferences and dislikes as well as daily routines. Personalised advertisement would then be the smallest resulting threat. Interpersonal surveillance \parencite{Trottier2017}, stalking \parencite{Lyndon2011} or potential blackmailing through acquired sensitive information are by far greater and current threats. Additionally, it is unclear from a data-analyst point of view how to deal with encounters of media objects that indicate illegal behaviour e.g littering, assault or theft. Should such incidents be reported to public authorities or not to respect the person's right to anonymity?\\

With that being said I personally think that the ethical discrepancy is legitimately continuously discussed in this context. Showing how the UGC and the anonymity of the authors is treated in terms of transparency from sides of the data-analysts as well as the goal a project pursues are key elements that determine in my opinion if ethical terms are violated or not. \\

The acquired data for this thesis is no exception, sensible information in the form of author names, locational information or timestamps referring to upload times or the time an image was taken are present. I am certain that the data in its entirety could be used to profile users by mapping their geo-tagged media objects to estimate places with high visitation probability. Furthermore, the sophisticated analyse of their personal statements contained in the user-generated text can reveal additional sensitive information such as birth-dates, wedding anniversaries, addresses, names of loved ones or pets which might reveal plausible passwords for brute-forcing attacks \parencite{Routh2018} or provide leverage for following social engineering attacks \parencite{Krombholz2015}. On these grounds, the used Instagram, Flickr and Foursquare data was treated anonymously which means that to no time during the data-processing connections to human identities were made. I did not encounter any dubious activity during the manual verification process of the data. Georeferenced images of military weaponry were the only sightings that raised questions about legal conflict which were dropped as I realised that the location of the military base is public and even visible on Google Maps. I tried to provide full transparency of all procedures and processing steps to ensure an ethical handling of the data for the indicated purpose.