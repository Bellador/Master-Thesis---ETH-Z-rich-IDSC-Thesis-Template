\chapter{Discussion} \label{discussion}

\section{Reflection on RQ 1}\label{discussion_rq1}

\textit{Does a text-classification machine learning model trained on a combination of text and image-data yield better results than the same model solely trained on text-data?}\\

This research questions will be addressed by discussing the model performances of M1 and M2 during training, testing and on unseen data as seen in the results. Furthermore, possible explanations for the observed performance changes will be stated.

\subsection{Model performances}

Comparing model performances during model-tuning (see M1 - table \ref{tab:m1_linearSVC_bestscores} and M2 - table \ref{tab:m2_linearSVC_bestscores}), model-testing on the entire training dataset (see figure \ref{tab:m1_m2_linearSVC_final_scores}) and model-evaluation on new data (see table \ref{precision_unseen_data}) revealed some inconsistency. M1 solely showed a better performance while fitting on fewer training points. This was the case during model-tuning due to the necessary testing subset which accounts for 25\% of the total available data. Besides that M2 showed a better final model performance as well as a better performance on new (unseen) data. This discrepancy suggests that the size of the training dataset used in this thesis was considerably small since 25\% more training data still led to an significant performance increase for both models but especially for M2. Nevertheless, the enhanced M2 performance on unseen data indicates that M2 possesses a better generalisation capability compared to M1 which is also reflected in the reduced Type II Error or False Negative predictions - indicating a higher M2 recall score. This claim is based on the correct identification of 937 recreational activities in Instagram and Flickr media objects by M2 compared to 451 by M1. The difference of 486 true positive predictions implies M2 yielded a \textbf{factor of two} more correctly classified media objects compared to M1.\\

The inclusion of image labels during training and media object prediction is sought to have helped the M2 recall improvement by identifying recreational activities that were not explicitly mentioned in the user-generated text. Drawing upon the additional information pool contained in the structural elements of the images favoured the detection of these additional media objects. On the other hand it was shown that M1 which was not trained on image labels showed a significantly worse performance (see table \ref{tab:m1_actual_precision}) if the image content information was provided in the final text-string used for prediction. Firstly, this indicates that the corpora vocabulary created by the bag-of-words algorithm already contained certain image labels that the Google Cloud Vision API returned, because otherwise these words would be ignored by the model and no effect would have been recorded. Secondly, this observation implies that simply providing further data in the form of text for the model to predict on does not simultaneously yield a performance increase but rather the opposite. Possible reason being that most media objects which contain a recreational activity are characterised by specific nature and environment related keywords since they are performed outdoors. Especially media objects that did not contain any user-generated text which are quite common especially on Flickr were observed to be falsely classified since the image labels that oftentimes include the mentioned keywords were the only data for prediction. This issue could easily be addressed by implementing a condition which classifies a media object as None if insufficient data for prediction is available. \\

\section{Reflection on RQ 2}\label{discussion_rq2}

\textit{Can social media derived data function as a proxy to estimate the spacial occurrence of NBRAs as a complementary source to conventional methods such as interviews and surveys?} \\

The spatial distribution maps of the considered recreational activities (see figure \ref{fig:map_cluster_1} and figure \ref{fig:map_cluster_2}) as M2 output and the interview as well as passive observation data (see section \ref{groud_truthing}) will be used as basis to discuss research question number two. \\
Before the proxy potential of VGI compared to empirical data is discussed, the observed difference between the two considered social media platforms Instagram and Flickr will be addressed.

\subsection{Comparing Instagram and Flickr}
A comparison of the spatial distributions of NBRA-related data points between the two social media platforms reveal that Flickr provides a relative homogeneous coverage compared to Instagram where clustering is more apparent across the research area. This observation is supported by the fewer unique locations of Instagram (152) compared to Flickr (1'144) as mentioned in section \ref{results_spatial_dist_model_NBRA} - the Instagram dataset being three times bigger than the Flickr dataset underlines this fact even further. The reason is linked to the way the data is georeferenced on the platforms. Flickr supports the extraction of exact coordinates from the image meta-data recorded by the camera or mobile device GPS and attaches it to the uploaded media object. Instagram on the other hand - due to regulatory data privacy policies - does not allow its users to share precise locations associated with their media objects but rather offers a predefined places-of-interest list with suggestions according to the current GPS location of the device. This restriction is likely to cause inconsistent location assignments by Instagram users. Variable place knowledge especially of tourists can lead to general and superordinate or even false assigned location tags that ultimately reduce the spatial mapping accuracy of the media object \parencite{Flatow2015}. Choosing suggested locations by the application for convenience instead of searching for more precise alternatives also effects the spatial accuracy of Instagram data. \\
The explanatory power of a data-source to describe spatial relationships is not only dependant on the accuracy of the individual media object but also on the total available amount of data which correlates with the social media platform popularity. Web-traffic associated with Instagram is multiple times bigger than from Flickr according to the Alexa Global Rank\footnote{https://www.alexa.com/topsites, accessed: 10.04.2019}. Correspondingly, I personally consider a combination of the two data-sources to yield a good balance of data-broadness and coverage provided by Instagram and spatial accuracy provided by Flickr.

\subsection{Model output compared to empirical data}
The broad comparison of observed recreational activity occurrences per class by the passive observation (which includes the occurrences recorded by the interviews) and the two social media platforms revealed that Flickr matched the ranking of the empirical data entirely. Instagram alone as well as the combination of Instagram and Flickr provided a significantly lower Spearman's Rank-Order Correlation of $\rho$=0.4 (see section \ref{results_comp_ground_truth}). This observation alone does not invalidate the credibility of Instagram data. Two of the seven considered recreational activities namely the classes 'picnic' and 'horse riding' were not recorded once in the ground truth. Additionally, the empirical data was acquired from three specific locations over a short period of time which was compared to VGI data from the entire research area that included media objects from 2004 until 2018. The exercising of outdoor activities is linked to specific landscape characteristics such as provided infrastructure, aesthetics and geographic closeness that favour certain places over others \parencite{Mancini2018a}. The occurrence probability of recreational activities is therefore expected to vary in space. This concludes that the result of this ranking holds limited validity to form general assumptions. At this point it must also be mentioned - as addressed under the \nameref{future_prospects} of section \ref{future_prospects} - that a sophisticated statistical spatial similarity analysis would have provided more insight on the topic compared to the above discussed ranking correlation as well as the following visual comparison of the maps. \\

The maps in figure \ref{fig:map_cluster_1} and figure \ref{fig:map_cluster_2} show the output of M2 and represent the spatial distribution of recreational activities in space. The visual comparison of the displayed data to the ground truth was performed in section \ref{results_comp_ground_truth} of the \nameref{results}. Observed inconsistency between the data-sources will be subsequently further discussed. \\
The class 'biking' shows much disagreement between the data-sources. Investigations revealed that the corresponding Flickr data mostly originated from one major cycling event in the year 2005 to which many users contributed. The densification of Instagram media objects in the urban area of the City of Zug is dominantly caused by false or general location tags such as 'Zug' or 'Canton Zug'. \\
According to the interviews was 'Schattenw\"aldli' the only of the three location were people stated to perform the recreation activity 'hiking'. VGI also indicates a strong signal in this location but also in the urban areas where one might not expect hiking to occur. The first assumption of people mixing between definitions of hiking and walking was proven wrong. The occurrences of as 'hiking' classified Instagram and Flickr media objects is mainly caused by false classifications of panorama photographs with mountainous landforms. \\
'Walking' as recreational activity was the recorded the most throughout all 52 interviews. This observation is most likely related to the data / interviewee sampling bias which is explained in section \ref{discussion_bias_ground_truth}. The ground truth data and the Instagram and Flickr signals coincide well. Manual verification proved reliable model precisions for both social media platforms.\\
The classification 'jogging' shows a moderate ground truth signal across all three locations which coincides well with the Flickr response. Instagram signals are almost exclusive to the core of the City of Zug. Plausible explanations are related to a below average model precision for the classification 'jogging' as further illustrated in section \ref{discussion_class_specific_perform}. \\
Similar ground truth signals to the classification 'jogging' are also recorded for 'dog walking'. The spatial distribution of Instagram and Flickr data-points show different patterns. The former displays a strong densification again in the core of the City of Zug which was proven to be related to false classifications of indoor instead of outdoor dog related media objects as well as general location tags. Flickr media objects are spread out without any dominant hot spots but cover the exact location of 'Schattenw\"aldli' which coincides with the strongest ground truth signal of this class. \\
The remaining recreational activities 'picnic' and 'horse riding' will not be discussed in regards to comparable ground truth signals since the data foundation is not given. How well the model derived recreational activity occurrences match classification specific infrastructure will be discussed further down in section \ref{discussion_foursquare}.\\

\subsection{Demographics}
An important aspect to assess the potential of Instagram and Flickr data to predict spatial relationships of recreational activities are the demographic properties of the corresponding social media platforms. Demographics indicate to what degree the data-source covers the actual variation of visitors in the research area and ultimately how representative compared to the entire population the data is. With the help of the ground truth it was determined that the average age across all 52 interviewees lies at 44. This includes social media users with an average age of 33 which matches Instagram's population description\footnote{https://napoleoncat.com/stats/instagram-users-in-switzerland/2018/10, accessed: 11.04.2019} and non social media users with an average age of 64 (see section \ref{fig:interview_age_SMP}). According to these results the elderly are slightly underrepresented by the social media data. Missing familiarity with electronic devices and the internet in general were recorded through the interviews as reasons for less social media engagement. 

\section{Recorded biases} \label{discussion_rec_bias}
Multiple different data-sources have been used in the course of this thesis. All data contain some form of bias given either by its origin, composition, the way it was obtained or processed. 
This section will cover encountered biases and their potential effects on the results as well as their interpretation.

\subsection{Ground truth} \label{discussion_bias_ground_truth}
The conventional data-acquisition methods such as surveys and interviews hold a comparably low reputation of bias compared to techniques working with social media data. Nevertheless, performing 52 interviews revealed certain conceptual biases that will be addressed here. The fact of only performing ground truthing during three days in the same season for only three specific locations in the entire research area will not be further discussed. The limited informative and representative value is undoubted but was constraint by the time expenditure for this task and the duration of this thesis. \\
One major observed problematic was encountered during the selection phase of a possible interviewee in the field. Depending on the activity that person was performing the chance of an active engagement attempt from the side of the author and an positive consent response from the interviewee varied strongly. People that were jogging with headphones for instance were less likely to be interviewed than a person going for a relaxing walk. It is without doubt that this discrepancy in interviewing probability per NBRA-class has an impact on the observed distribution.\\
Secondly, it was noted that to avoid long pauses - where the interviewee gets stuck on a question - certain words, ideas or examples were mentioned to keep the flow of the interview alive. It often occurred that the interviewees were convinced by some of the mentioned words which were then given as answers. These were mostly keywords that described the visitation motivation such as the term \textit{geographic closeness} which occurred in higher frequencies than others. These depicted biases will reflect in the dataset used for evaluating the models legitimacy and the ground truth results. Similar biases were also observed by \textcite{Hanemann2011, Kling2012, Tenerelli2016} regarding the procedure of traditional surveys and interviews.

\subsection{Model}
It is known that data from social media platforms such as Instagram and Flickr contain various forms of bias given among others by the demography of the user base \parencite{Heikinheimo2017}, the platform purpose, the dominantly uploaded content-type, noise by bots that produce delusive content \parencite{Edwards2014} and fluctuations in social media platform popularity. These general social media data biases are already widely covered in scientific literature \parencite{Ruths2014, Lazer2014, Zook2017}. Data-processing and model setup induced bias specific to the approach used in this thesis will subsequently be addressed and discussed. \\

\subsubsection{Data-processing} Dataset integrated bias such as dominant users and bulk-uploads that distort the general perception of the data were mitigated by subjectively defined and implemented thresholds during the data-processing phase described in section \ref{bias_dominant_authors} and \ref{bias_bulk_uploads} respectively. These thresholds as already mentioned are set by the author which simultaneously adds a subjective view and alteration to every dataset. Also, it is to be expected that these are not the only biases present. Media objects for instance by tourists and locals were treated with equal importance, even though the latter should have a better knowledge over the optimal places to perform certain activities. Additionally, media objects which try to promote e.g. a product, a location, an activity or which are influenced or sponsored by a third party were not specifically filtered out.\\

\subsubsection{Training data} The composition, origin and quantity of the training data is linked to numerous known flaws of which the majority were investigated. All findings will be listed and discussed here. \\
The fact that both models were solely trained on Instagram data but used to predict also Flickr media objects is a known bias. The associated magnitude of this flaw in regards to the performance of M1 and M2 was investigated in table \ref{tab:m1_actual_precision} and table \ref{tab:m2_actual_precision} respectively and proven to be not significant on a 5\% confidence level. \\
The issue of using media objects for training that originate from a different area than the media objects on which the models predict is problematic and doubtful. This decision was not made by choice but rather because of a too small available dataset from the research area. The short data-acquisition period (see section \ref{Instagram_timespan}) and the deprecation of the Instagram API on the 11.12.2018 did not allow for enough media objects per NBRA-class to be acquired to train a sophisticated model. The absence of a significant language difference between the two regions which is considered to be one of the crucial requirements was investigated in section \ref{fig:det_languages} and could not be disproven. Even with the multiple times bigger dataset from Zurich were some classes such as 'horse riding' with 13 entries (see table \ref{tab:trainingsdata}) poorly represented. This is not a bias in its usual sense but rather a weakness of the model as a whole. \\
Lastly, the procedure used to create the training dataset containing a subset of 1'046 media objects from the database table \texttt{media\_objects\_trainingdata\_instagram} was subject to manually constructed SQL queries that contained NBRA-specific keywords (see appendix \ref{sql_queries_for_trainingdata}) such as action verbs, nouns describing commonly used equipment as well as associated hashtags. These keywords were partly based on exemplary media objects of the given class found in the dataset as well as on the official Instagram application but also on the intuition of the author. This implies that the training data is only as good as the used SQL queries which are prone to missing certain facets of NBRA media object characteristics. It cannot be ruled out that user groups that perform a specific variation of a recreational activity are excluded from the training data and are therefore not represented. The effect of this selection process on the resulting model features is discussed further down in section \ref{discussion_model_features}.

\subsubsection{Setup} 
The classifications between which the models differentiate were chosen based on the guideline from \textcite{IFL2018}. The manual training data selection as well as the manual model performance evaluation revealed how alike certain classes are. This similarity was substantiated by various found interpretations of media object authors on what they think a certain recreational activity is. The strongest inconsistency is observed between the classes 'walking' and 'hiking'. The implementation of one's own definition of these classes is of no use if the content of the media objects do not follow them. The phrase: "I went for a walk on the mountain Matterhorn." should illustrate this problem nicely. A potential author of that phrase obviously described his/her performed activity as 'walking' according to the used verb and his/her perception. Somebody else would associate this phrase rather with the activity 'hiking' due to the mentioned noun 'mountain'. At this point the question arises what should be valued more - the perception of the original author or a set definition by the person evaluating the data? Chances are that the original author possesses more information about the situation that is not included in the media object itself. This information might explain why the verb 'walking' was chosen over 'hiking' e.g. because a mountain railway was used. 
Taking this into consideration a merge of these two NBRA-classes could be considered.

\subsubsection{Predictions}
The spatial distribution accuracy and the associated interpretation bias of Instagram media objects has been mentioned in section \ref{instagram_location_tag}. User generated location tags impose the issue of imprecise or simply wrong positions as also described by \textcite{Lee2016}. To mitigate these effects superordinate, general location tags associated with a specific region could be excluded as performed by \textcite{Heikinheimo2017} to increase the overall spatial distribution accuracy of NBRAs for the remaining data-points.

\section{Model components} \label{disussion_model_performance}
The different test-arrangements and the components that make up the final models M1 and M2 will be further discussed in the following subsections. This includes evaluation of the Random Forest algorithm and linearSVC used for model-fitting, model feature characteristics and expressiveness as well as the discussion of classification specific model precision scores on unseen data.

\subsection{Evaluation scores}
Unlike common practice \parencite{Guido2016} both models M1 and M2 were tuned according to the average F1-score across all classes except the None-class. The optimal hyperparameter setting was therefore chosen to maximise the model performance of the recreational activity classes. The reasoning behind this choice was given by the heavily asymmetric availability of training data per class. Media objects resembling the None-class amounted to 99.6\% of the entire \texttt{media_objects_trainingdata_instagram} database table which left 0.4\% for the training of the recreational classes (see table \ref{tab:trainingsdata}).

\subsection{Fitting algorithms}
The linear support vector classifier (linearSVC) outperformed the decision tree based Random Forest fitting algorithm for this text-classification task as proclaimed by \parencite{Guido2016}. It is assumed that the linearSVC was able to describe the large feature-space better due to the kernel trick that allows the algorithm to find the most optimal hyperplane in any higher dimensional space to maximises the distance between points of different classes \parencite{Shawe-Taylor2004}.

\subsection{Features} \label{discussion_model_features}
The features associated with a model or more specifically with a model's classes give insight into its inner workings which otherwise seems like a black box. Figure \ref{fig:M2_top40_features_biking} and Appendix \ref{M2_top_features} provide information about the most descriptive features of every M2 classification by linking word tokens (features) to coefficient magnitudes. This value indicates the strength or importance a word is associated with if it appears in a given media object text. The features of the class 'None' for instance provide a good impression on trending topics such as \textit{bodybuilding}, \textit{tattoo} or \textit{coffee} circulating in the Instagram community of the region Zurich at the time the training data was acquired. It can also be observed that action verbs such as \textit{run} or \textit{jog} for the class 'jogging' as well as \textit{walk} or \textit{stroll} for the class 'walking' are often among the top three features. It is without doubt that these word tokens hold a lot of descriptive significance when it comes to identifying a media object with those mentioned recreational activities. The model learned and identified these features based on the provided training data. But one has to keep in mind that this training data was acquired exactly with tailored SQL queries on the database that contained and filtered among others for exactly these action verbs. This shall signalise that the appearance of these tokens as top features is partly given by the direct model learning process but also by the way the training data was prepared. \\ 

Coming back to the feature \textit{run} that holds the highest descriptive power or coefficient magnitude for the class 'jogging' is in the English language not exclusively used in the context of a person running but moreover in many different linguistic compositions, phrases or sayings. \textit{To run errands, to run late} or something is \textit{run-down} are examples that illustrate this variety of use cases. This issue is expected to be the main cause for the significantly low model performance of the class 'jogging' which is again mentioned in section \ref{discussion_class_specific_perform}. For the model to be able to comprehend and differentiate between these context specific use cases it demands significantly more training data than provided in this thesis. 

\subsection{Classification specific scores}\label{discussion_class_specific_perform}
The manually performed M1 and M2 performance verification on unseen data revealed strong differences between classification-specific precision scores (see M1 - figure \ref{fig:M1_precision_unseen_data} and M2 - figure \ref{fig:M2_precision_unseen_data}). Observed and plausible causes for these differences will subsequently be discussed.
One major issue that effects model performance are classes with similar derived features. The class 'walking' and 'dog walking' are obvious candidates since they share their descriptive action verb \textit{to run}. This problem is hard to be taken care of because it lies in the nature of the classes and their associated recreational activity.
Additionally, features with different associations or use cases as already mentioned in section \ref{discussion_model_features} can reduce a classes performance significantly if they hold high explanatory power as in the case of 'jogging' and the verb \textit{run} which is also responsible for the observable, below average precision. Reasons for the similarly poor performance of the class 'picnic' are related to the specific setting of eating a meal outdoors in nature. The conditions \textit{eating}, \textit{outdoor} and \textit{nature} have to be simultaneously fulfilled in order for a media object to be classified as 'picnic'. Investigations showed that most false classifications were media objects with food images of restaurant visits with view. Accordingly, often times nature related hashtags or descriptions were used to describe the image which frequently contained landscape elements. This arrangement lead the model to associate the media object to 'eating' and 'outdoor' which caused the false classification. A similar difficulty can be observed for the class 'dog walking'. Loss in performance was mostly related to indoor photographs of dogs which are not considered proxies of the recreational activity. The model was again not entirely able to check for all needed conditionals which encompass the presence of the animal dog, signs of the activity walking as well as evidence of outdoor related elements. 

It was additionally observed that hiking

- hiking -> pictures of mountains wrongly classified
- walking -> "walk on water" -> clearly wrong classification (see results)

- dog walking -> just images of dogs indoors
- biking -> bike shops 
- dog walking --> comprehend: outdoor content is not simultaniously recreational activity related

\subsection{Hyperparameter configurations}

If different hyperparameter settings produce the same model performance (F1-score) than the setting which produces fewer features is prioritised due to better generalisation and regularisation capabilities. Regarding figure \ref{fig:m1_heatmap} for instance the \textit{ngram\_range} setting of either (1, 1), (1, 2) or (1, 3) in combination with \textit{C} = 1 all produce a F1-score of 0.932. (1, 1) has been chosen as best hyperparameter configuration due to the lowest feature count.

\section{Comparison to model baselines} \label{model_baseline}
The models presented in this thesis can be compared to text-classification models of similar research papers to acquire a baseline reference which were also introduced in the State of the Art chapter under section \ref{existing_text_class_models}. \\
Recent work of \textcite{Das2018} presents a text sentiment classification model which is based on NF-IDF (also used in this thesis) in conjunction with an additional algorithm to enhance sentiment prediction. Three different classifiers were tested of which also the linearSVC showed the best performance on used review datasets. The presented 10-fold cross-validated model accuracy on the main IMBD movie review dataset was 89.91\% with a train-test split of 80\% and 20\% respectively.\\
Another reference is the deep-learning based model created by \textcite{Li2018} which is optimised to classify Chinese text. The observed accuracy of the tuned model on the Chinese and English dataset lies at 96.23\% and 94.88\% respectively. The latter should function as better comparison to M1 and M2 from this thesis which achieved a test accuracy of 93.7\% and 88.2\% respectively.

\section{Allocation of recreational infrastructure} \label{discussion_foursquare}
xxxx FOURSQUARE xxxx

\section{Media object languages}
xxxxxxx Shortly explain what the graph shows xxxx
A lot of hashtags used across different languages are primarily in English. Therefore, even a media object which in its core was not written exclusively in English can be classified as English by the algorithm due to the present hashtags. Consider the following exemplary German media object: \\ \textit{"Wandern tut gut \#view \#freshair \#goodlifestyle \#health"} (hiking feels good) \\ This media object would be classified as English since there are four English hashtags and only three German words of the actual sentence.

\section{Data availability}
The social media platform related regulations, policies and APIs are always subject to change and can effect the future data-composition and data-availability. as well as studies that try to use them as data-source.

Regarding reproducability of this work
- people can change their content from puplic to private or delete




\section{Ethics}
Drawing upon crowed sourced big-data is inevitably shadowed by ethical controversies as shown by \textcite{Taylor2018}. Using GPS locations and other sensitive data to identify and analyse patterns of human behaviour is rightfully criticised and sometimes considered unethical practise \parencite{Taylor2018} which is why ethical guidelines for digital engagement are of increasing importance \parencite{Bowen2013}.\\
Normally, when we talk about UGC we are referring to open-data that is publicly available, data that the user decided to share on the web on his or her own behalf - or is it?. The paper of \textcite{Estima2016} shows that not the entirety of UGC is voluntarily and willingly shared. Did the user sign up for his or her data to be systematically analysed? This depends on the Terms of Service (TOS) of the corresponding service which are hardly ever read by the consumer.\\
Media appearances such as Facebook's experiments in emotion manipulation \parencite{Jouhki2016} which enabled the targeting of easily susceptible teenagers to personalised advertisement\footnote{https://www.independent.co.uk/news/media/facebook-leaked-documents-research-targeted-insecure-youth-teenagers-vulnerable-moods-advertising-a7711551.html, accessed: 07.04.2019} present the power this data possess. Having access to hundreds of personalised media objects of a specific social media user enables analytics to construct a blueprint of that person. This blueprint can encompasses the persons political orientation\footnote{https://www.theguardian.com/media/2010/apr/30/social-media-election-2010, accessed: 07.04.2019}, its home and work location, preferences and dislikes as well as daily routines. Personalised advertisement would then be the smallest resulting threat. Interpersonal surveillance \parencite{Trottier2017}, stalking \parencite{Lyndon2011} or potential blackmailing through acquired sensitive information are by far greater and current threats. Additionally, it is unclear from a data-analyst point of view how to deal with encounters of media objects that indicate illegal behaviour e.g littering, assault or theft. Should such incidents be reported to public authorities or not to respect the person's right to anonymity?\\

With that being said I personally think that the ethical discrepancy is legitimately continuously discussed in this context. Showing how the UGC and the anonymity of the authors is treated in terms of transparency from sides of the data-analysts as well as the goal a project pursues are key elements that determine in my opinion if ethical terms are violated or not. \\

The acquired data for this thesis is no exception, sensible information in the form of author names, locational information or timestamps referring to upload times or the time an image was taken are present. I am certain that the data in its entirety could be used to profile users by mapping their geo-tagged media objects to estimate places with high visitation probability. Furthermore, the sophisticated analyse of their personal statements contained in the user-generated text can reveal additional sensitive information such as birth-dates, wedding anniversaries, addresses, names of loved ones or pets which might reveal plausible passwords for brute-forcing attacks \parencite{Routh2018} or provide leverage for following social engineering attacks \parencite{Krombholz2015}. On these grounds, the used Instagram, Flickr and Foursquare data was treated anonymously which means that to no time during the data-processing connections to human identities were made. I did not encounter any dubious activity during the manual verification process of the data. Georeferenced images of military weaponry were the only sightings that raised questions about legal conflict which were dropped as I realised that the location of the military base is public and even visible on Google Maps. I tried to provide full transparency of all procedures and processing steps to ensure an ethical handling of the data for the indicated purpose.