\chapter{Discussion} \label{discussion}

\section{Recorded biases}
Multiple different data-sources have been used in the course of this thesis. All data contain some form of bias given either through its origin, composition, the way it was obtained or processed. 
This subchapter will cover encountered biases and their potential effects on the results as well as their interpretation.

\subsection*{Ground truth}
The conventional data-acquisition methods such as surveys and interviews hold a comparably low reputation of bias compared to the more recent techniques working with SMD. Performing 52 interviews revealed certain conceptual biases that will be shortly addressed here. The fact of only performing ground truthing during three days in the same season for only three locations in the entire research area will not be further discussed here. The limited informative and representative value is undoubted but was constraint by the time expenditure for this task and the duration of this thesis. \\
\newline
One major observed problematic was encountered during the selection phase of a possible interviewee in the field. Depending on the activity that person was performing the chance of an active engagement attempt from the side of the author and an positive consent response from the other party varied strongly. People that were jogging for instance with headphones on were less likely to be interviewed than a person going for an relaxing walk. It is without doubt that this discrepancy in interviewing probability per NBRA-class has an impact on the observed distribution.\\
Secondly, it was noted that to avoid long pauses - where the interviewee gets stuck on a question - certain words, ideas or examples were mentioned to keep the flow of the interview alive. It often occurred that the interviewees were convinced by some of the mentioned words and where therefore noted as response. These were mostly keywords that described the visitation motivation such as 'geographic closeness' and the observed frequencies were noticeably higher than others. These depicted biases will reflect in the dataset used for evaluating the models legitimacy and the ground truth results.

\subsection*{Model}
It is known that SMD from SMP such as \textit{Instagram} and \textit{Flickr} contain various forms of bias given by among others the demography of the user base, the platform purpose and the dominantly uploaded content-type. These biases are already widely covered in scientific literature such as \parencite{} and will not be further discussed here since they can hardly be controlled besides the choice of the used SMPs themselves. Data-processing and model setup induced bias however will be addressed since it is specific to this thesis. \\

\paragraph*{Data-processing:} SMP-dataset integrated bias such as dominant users and bulk-uploads that distort the general perception of the data were mitigated by subjectively defined and implemented thresholds during the data-processing phase described in sections \ref{bias_dominant_authors} and \ref{bias_bulk_uploads} respectively. These thresholds as already mentioned are set by the author which simultaneously adds a subjective view and alteration to every dataset. Also, it is to be expected that these are not the only biases present. Media objects for instance by tourists and locals were treated with equal importance, even though the latter should have a better knowledge over the optimal places to perform certain activities. Additionally, media objects which try to promote e.g. a product, a location, an activity or which are influenced or sponsored by a third party were not filtered out.\\
\paragraph*{Training's data:} The composition, origin and quantity of the training's data is linked to numerous known flaws of which the majority were investigated. They will be listed and discussed here in their entirety. \\
The fact that both models were solely trained on \textit{Instagram} data but used to predict also \textit{Flickr} media objects is a known bias. The associated magnitude of this flaw in regards to the performance of M1 and M2 was investigated in table \ref{tab:m1_actual_accuracy} and \ref{tab:m2_actual_accuracy} respectively and proven to be not significant on a 5\% level. \\
The issue of using media objects for training that originate from a different area than the media objects on which the models predicts is problematic and doubtful. This decision was not made by choice but rather because of a too small available dataset from the research area. The short data-acquisition period (see section \ref{Instagram_timespan}) and the retirement of the \textit{Instagram} API on the 11.12.2018 did not allow for enough media objects per NBRA-class to train a sophisticated model. Even with the multiple times bigger dataset from Zurich were some classes such as 'horse riding' with 13 entries (see table \ref{tab:trainingsdata}) extremely weakly represented. This is not a bias in its usual sense but rather a weakness of the model as a whole.

\paragraph*{Setup:} 
The classifications between which the models differentiate were chosen based on the guideline from \parencite{IFL2018}. The manual training's data selection as well as the manual model performance evaluation revealed how alike certain classes are. This similarity was mostly given by various interpretation of observed media object authors on what they think a certain NBRA is. The strongest inconsistency is observed in between the classes 'walking' and 'hiking'. The implementation of ones own definition of these classes is no use if the content of the media objects which the models predicts do not follow them. The phrase: "I went for a walk on the mountain Matterhorn." should illustrate this problem nicely. A potential author of that phrase obviously described his/her performed activity as 'walking' according to the used verb and his/her perception. Somebody else would associate this phrase rather with the activity 'hiking' due to the mentioned noun 'mountain'. At this point the question arises what should be valued more - the perception of the original author or a set definition by the person evaluating the data? Chances are that the original author processes more information about the situation that is not included in the media object which might explain why the verb 'walking' was chosen over 'hiking' e.g. because a mountain railway was used. 
Taking this into consideration a merge of these two NBRA-classes can be considered for future applications of this method. 

\paragraph*{Predictions}
The spatial distribution accuracy of \textit{Instagram} media objects has been discussed in section \ref{instagram_location_tag}. User generated location tags impose the issue of imprecise or simply wrong positions. To mitigate the effects superordinate and general location tags 

argue if classifications were to specific 

location: could have filtered out locations which referred to the entire research region, similar to \parencite{Heikinheimo2017}


- aussagekraft der r√§umlichen Verteilung der Instagram daten aufgrund der allgemeinen Location-tags.


